{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iu7kkaTQNWOz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anandafrancis/ananda/c-resources/d-leverage/credentials/accreditation/degrees/northeastern/fa25/thesis/evolutionary-ml/venv-arm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/anandafrancis/ananda/c-resources/d-leverage/credentials/accreditation/degrees/northeastern/fa25/thesis/evolutionary-ml/venv-arm/lib/python3.11/site-packages/torch/cuda/__init__.py:58: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import librosa\n",
        "import subprocess\n",
        "import io\n",
        "import soundfile as sf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from transformers import (AutoFeatureExtractor, MCTCTFeatureExtractor, ParakeetFeatureExtractor, SeamlessM4TFeatureExtractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iowzEHbKNgiV"
      },
      "outputs": [],
      "source": [
        "class Song:\n",
        "    def __init__(self, name, audio_vector, lang, samplerate, length, split):\n",
        "        self.name = name\n",
        "        self.audio = audio_vector\n",
        "        self.lang = lang\n",
        "        self.samplerate = samplerate\n",
        "        self.length = length\n",
        "        self.split = split\n",
        "\n",
        "    def _resample_audio(self, target_sr):\n",
        "        return librosa.resample(y=self.audio, orig_sr=self.samplerate, target_sr=target_sr)\n",
        "\n",
        "    def extract_all_features(self):\n",
        "        \"\"\"Extract ALL features at once, then delete audio\"\"\"\n",
        "\n",
        "        # intialize\n",
        "        features = {}\n",
        "\n",
        "        # Traditional features\n",
        "        resample_12k = self._resample_audio(12000)\n",
        "        features['stft'] = librosa.stft(y=resample_12k, n_fft=512, hop_length=256)\n",
        "        features['mel_specs'] = librosa.feature.melspectrogram(y=resample_12k, sr=12000, hop_length=256, n_fft=512, n_mels=96)\n",
        "        features['mfccs'] = librosa.feature.mfcc(y=resample_12k, sr=12000, hop_length=256, n_fft=512)\n",
        "        del resample_12k\n",
        "        gc.collect()\n",
        "\n",
        "        # Learned features\n",
        "        resample_16k = self._resample_audio(16000)\n",
        "\n",
        "        # MCTCT\n",
        "        feature_extractor = MCTCTFeatureExtractor()\n",
        "        features['mctct'] = feature_extractor(raw_speech=resample_16k, sampling_rate=16000, return_tensors=\"pt\")['input_features'][0]\n",
        "        del feature_extractor\n",
        "        gc.collect()\n",
        "\n",
        "        # Parakeet\n",
        "        feature_extractor = ParakeetFeatureExtractor()\n",
        "        features['parakeet'] = feature_extractor(resample_16k, sampling_rate=16000, return_tensors=\"pt\")['input_features'][0]\n",
        "        del feature_extractor\n",
        "        gc.collect()\n",
        "\n",
        "        # SeamlessM4T\n",
        "        feature_extractor = SeamlessM4TFeatureExtractor.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n",
        "        features['seamlessM4T'] = feature_extractor(resample_16k, sampling_rate=16000, return_tensors=\"pt\")['input_features'][0]\n",
        "        del feature_extractor\n",
        "        gc.collect()\n",
        "\n",
        "        # Whisper\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"sanchit-gandhi/whisper-medium-fleurs-lang-id\")\n",
        "        features['whisper'] = feature_extractor(resample_16k, sampling_rate=16000, return_tensors=\"pt\")['input_features'][0].T\n",
        "        del feature_extractor\n",
        "        gc.collect()\n",
        "\n",
        "        # NOW delete audio\n",
        "        del self.audio\n",
        "        self.audio = None\n",
        "        gc.collect()\n",
        "\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z4sp_A2ARZZk"
      },
      "outputs": [],
      "source": [
        "def process_song(path, lang, sr=22050, clip_len=15):\n",
        "    \"\"\"Process a single song into clips\"\"\"\n",
        "    try:\n",
        "        y, _ = librosa.load(path, sr=sr, mono=True)\n",
        "    except Exception:\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                ['ffmpeg', '-v', 'error', '-nostdin', '-i', path,\n",
        "                 '-ac', '1', '-ar', str(sr), '-f', 'wav', '-'],\n",
        "                check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "            )\n",
        "            wav_bytes = io.BytesIO(proc.stdout)\n",
        "            y, _ = sf.read(wav_bytes, dtype='float32')\n",
        "            if y.ndim > 1:\n",
        "                y = np.mean(y, axis=1)\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    name = os.path.splitext(os.path.basename(path))[0].lower()\n",
        "\n",
        "    if y is None or len(y) == 0:\n",
        "        return []\n",
        "\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "    if duration < 120:\n",
        "        return []\n",
        "\n",
        "    center = duration / 2\n",
        "    start = int((center - 60) * sr)\n",
        "    end = int((center + 60) * sr)\n",
        "    y = y[start:end]\n",
        "\n",
        "    clip_size = clip_len * sr\n",
        "    clips = [y[i:i + clip_size] for i in range(0, len(y), clip_size)\n",
        "             if len(y[i:i + clip_size]) == clip_size]\n",
        "\n",
        "    if len(clips) == 0:\n",
        "        return []\n",
        "\n",
        "    split_assignments = ['train', 'test', 'train', 'validation', 'train', 'test', 'train', 'train']\n",
        "\n",
        "    return [Song(name, clip, lang, sr, clip_len, split_assignments[i])\n",
        "            for i, clip in enumerate(clips[:len(split_assignments)])]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAUtdysLRg30"
      },
      "outputs": [],
      "source": [
        "def build_dataset_by_language_batch():\n",
        "    \"\"\"\n",
        "    Process languages in batches with INCREMENTAL SAVING.\n",
        "    Saves after each language to prevent data loss on crashes.\n",
        "    Can resume from checkpoint.\n",
        "    \"\"\"\n",
        "\n",
        "    languages = ['patois', 'mandarin', 'english', 'spanish', 'hindi', 'pidgin']\n",
        "    feature_types = ['stft', 'mel_specs', 'mfccs', 'mctct', 'parakeet', 'seamlessM4T', 'whisper']\n",
        "    \n",
        "    # Checkpoint file to track progress\n",
        "    checkpoint_file = 'preprocessing_checkpoint.json'\n",
        "    completed_languages = []\n",
        "    \n",
        "    # Load checkpoint if exists\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        import json\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            completed_languages = checkpoint.get('completed_languages', [])\n",
        "            print(f\"Resuming from checkpoint. Already completed: {completed_languages}\")\n",
        "    \n",
        "    # Storage for current language only (not all languages)\n",
        "    lang_features = {\n",
        "        'train': {feat: [] for feat in feature_types},\n",
        "        'validation': {feat: [] for feat in feature_types},\n",
        "        'test': {feat: [] for feat in feature_types}\n",
        "    }\n",
        "    lang_labels = {'train': [], 'validation': [], 'test': []}\n",
        "\n",
        "    # Process each language separately\n",
        "    for lang_idx, lang in enumerate(languages):\n",
        "        # Skip if already completed\n",
        "        if lang in completed_languages:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Skipping {lang} (already completed)\")\n",
        "            print(f\"{'='*60}\")\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing language {lang_idx+1}/{len(languages)}: {lang}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Reset for this language\n",
        "        for split in ['train', 'validation', 'test']:\n",
        "            for feat in feature_types:\n",
        "                lang_features[split][feat] = []\n",
        "        lang_labels = {'train': [], 'validation': [], 'test': []}\n",
        "\n",
        "        folder = f'./{lang}'\n",
        "        files = [f for f in os.listdir(folder) if f.endswith('.mp3')]\n",
        "\n",
        "        for file_idx, file in enumerate(files):\n",
        "            path = os.path.join(folder, file)\n",
        "\n",
        "            try:\n",
        "                clips = process_song(path, lang)\n",
        "\n",
        "                for clip in clips:\n",
        "                    # Extract ALL features at once\n",
        "                    all_clip_features = clip.extract_all_features()\n",
        "\n",
        "                    # Distribute features to their respective lists\n",
        "                    for feat_name, feat_data in all_clip_features.items():\n",
        "                        lang_features[clip.split][feat_name].append(feat_data)\n",
        "\n",
        "                    del all_clip_features\n",
        "\n",
        "                    lang_labels[clip.split].append(clip.lang)\n",
        "                    del clip\n",
        "\n",
        "                del clips\n",
        "                gc.collect()\n",
        "\n",
        "                if file_idx % 10 == 0:\n",
        "                    print(f\"  {lang}: {file_idx}/{len(files)} files\", end='\\r')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n✓ Completed processing {lang}\")\n",
        "        \n",
        "        # SAVE IMMEDIATELY after each language to prevent data loss\n",
        "        print(f\"Saving {lang} features incrementally...\")\n",
        "        try:\n",
        "            # Save this language's features to temporary files\n",
        "            for feat in feature_types:\n",
        "                lang_data = {\n",
        "                    'train_features': np.array(lang_features['train'][feat]) if lang_features['train'][feat] else np.array([]),\n",
        "                    'val_features': np.array(lang_features['validation'][feat]) if lang_features['validation'][feat] else np.array([]),\n",
        "                    'test_features': np.array(lang_features['test'][feat]) if lang_features['test'][feat] else np.array([])\n",
        "                }\n",
        "                \n",
        "                # Save to language-specific file\n",
        "                lang_filename = f'{feat}_features_{lang}.npz'\n",
        "                np.savez_compressed(lang_filename, **lang_data)\n",
        "                \n",
        "                del lang_data\n",
        "                gc.collect()\n",
        "            \n",
        "            # Save this language's labels\n",
        "            lang_labels_data = {\n",
        "                'train_labels': lang_labels['train'],\n",
        "                'val_labels': lang_labels['validation'],\n",
        "                'test_labels': lang_labels['test']\n",
        "            }\n",
        "            np.savez_compressed(f'labels_{lang}.npz', **lang_labels_data)\n",
        "            del lang_labels_data\n",
        "            gc.collect()\n",
        "            \n",
        "            # Update checkpoint\n",
        "            completed_languages.append(lang)\n",
        "            checkpoint = {'completed_languages': completed_languages}\n",
        "            import json\n",
        "            with open(checkpoint_file, 'w') as f:\n",
        "                json.dump(checkpoint, f, indent=2)\n",
        "            \n",
        "            print(f\"✓ Saved {lang} features and updated checkpoint\")\n",
        "            \n",
        "            # Clear memory\n",
        "            for split in ['train', 'validation', 'test']:\n",
        "                for feat in feature_types:\n",
        "                    del lang_features[split][feat]\n",
        "                    lang_features[split][feat] = []\n",
        "            gc.collect()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving {lang}: {e}\")\n",
        "            raise  # Re-raise to stop and fix issue\n",
        "\n",
        "    # Now combine all language files into final feature files\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Combining all languages into final feature files...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Accumulate all languages\n",
        "    all_features = {\n",
        "        'train': {feat: [] for feat in feature_types},\n",
        "        'validation': {feat: [] for feat in feature_types},\n",
        "        'test': {feat: [] for feat in feature_types}\n",
        "    }\n",
        "    all_labels = {'train': [], 'validation': [], 'test': []}\n",
        "    \n",
        "    for lang in completed_languages:\n",
        "        print(f\"Loading {lang}...\")\n",
        "        for feat in feature_types:\n",
        "            lang_filename = f'{feat}_features_{lang}.npz'\n",
        "            if os.path.exists(lang_filename):\n",
        "                lang_data = np.load(lang_filename, allow_pickle=True)\n",
        "                if len(lang_data['train_features']) > 0:\n",
        "                    all_features['train'][feat].append(lang_data['train_features'])\n",
        "                if len(lang_data['val_features']) > 0:\n",
        "                    all_features['validation'][feat].append(lang_data['val_features'])\n",
        "                if len(lang_data['test_features']) > 0:\n",
        "                    all_features['test'][feat].append(lang_data['test_features'])\n",
        "                del lang_data\n",
        "        \n",
        "        # Load labels\n",
        "        lang_labels_file = f'labels_{lang}.npz'\n",
        "        if os.path.exists(lang_labels_file):\n",
        "            lang_labels_data = np.load(lang_labels_file, allow_pickle=True)\n",
        "            all_labels['train'].extend(lang_labels_data['train_labels'])\n",
        "            all_labels['validation'].extend(lang_labels_data['val_labels'])\n",
        "            all_labels['test'].extend(lang_labels_data['test_labels'])\n",
        "            del lang_labels_data\n",
        "        gc.collect()\n",
        "    \n",
        "    # Concatenate and save final files\n",
        "    print(\"\\nConcatenating and saving final feature files...\")\n",
        "    for feat in feature_types:\n",
        "        try:\n",
        "            final_data = {\n",
        "                'train_features': np.concatenate(all_features['train'][feat], axis=0) if all_features['train'][feat] else np.array([]),\n",
        "                'val_features': np.concatenate(all_features['validation'][feat], axis=0) if all_features['validation'][feat] else np.array([]),\n",
        "                'test_features': np.concatenate(all_features['test'][feat], axis=0) if all_features['test'][feat] else np.array([])\n",
        "            }\n",
        "            \n",
        "            filename = f'{feat}_features.npz'\n",
        "            np.savez_compressed(filename, **final_data)\n",
        "            print(f\"✓ Saved {feat}: train={final_data['train_features'].shape}, val={final_data['val_features'].shape}, test={final_data['test_features'].shape}\")\n",
        "            \n",
        "            del final_data\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving {feat}: {e}\")\n",
        "    \n",
        "    # Encode and save final labels\n",
        "    all_labels_flat = all_labels['train'] + all_labels['validation'] + all_labels['test']\n",
        "    encoder = LabelEncoder()\n",
        "    y_encoded = encoder.fit_transform(all_labels_flat)\n",
        "    y_onehot = to_categorical(y_encoded)\n",
        "    \n",
        "    train_size = len(all_labels['train'])\n",
        "    val_size = len(all_labels['validation'])\n",
        "    \n",
        "    labels_data = {\n",
        "        'labels_inorder': list(encoder.classes_),\n",
        "        'train_labels': y_onehot[:train_size],\n",
        "        'val_labels': y_onehot[train_size:train_size+val_size],\n",
        "        'test_labels': y_onehot[train_size+val_size:]\n",
        "    }\n",
        "    \n",
        "    np.savez_compressed(f'labels.npz', **labels_data)\n",
        "    print(f\"\\n✓ Saved labels: {len(encoder.classes_)} classes\")\n",
        "    \n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'feature_types': feature_types,\n",
        "        'languages': list(encoder.classes_),\n",
        "        'splits': {'train': train_size, 'val': val_size, 'test': len(all_labels['test'])}\n",
        "    }\n",
        "    \n",
        "    import json\n",
        "    with open(f'metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    # Clean up temporary language-specific files\n",
        "    print(\"\\nCleaning up temporary language-specific files...\")\n",
        "    for lang in completed_languages:\n",
        "        for feat in feature_types:\n",
        "            lang_filename = f'{feat}_features_{lang}.npz'\n",
        "            if os.path.exists(lang_filename):\n",
        "                os.remove(lang_filename)\n",
        "        lang_labels_file = f'labels_{lang}.npz'\n",
        "        if os.path.exists(lang_labels_file):\n",
        "            os.remove(lang_labels_file)\n",
        "    \n",
        "    # Remove checkpoint\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        os.remove(checkpoint_file)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"✓ All features saved and temporary files cleaned up!\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'metadata' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmetadata\u001b[49m\n",
            "\u001b[31mNameError\u001b[39m: name 'metadata' is not defined"
          ]
        }
      ],
      "source": [
        "metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GgGlaOgRlUx",
        "outputId": "a4a465c3-808b-411f-ac04-2c8546777738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Colab-optimized preprocessing...\n",
            "\n",
            "============================================================\n",
            "Processing language 1/6: patois\n",
            "============================================================\n",
            "  patois: 100/105 files\n",
            "✓ Completed patois\n",
            "\n",
            "============================================================\n",
            "Processing language 2/6: mandarin\n",
            "============================================================\n",
            "  mandarin: 90/95 files\n",
            "✓ Completed mandarin\n",
            "\n",
            "============================================================\n",
            "Processing language 3/6: english\n",
            "============================================================\n",
            "  english: 90/93 files\n",
            "✓ Completed english\n",
            "\n",
            "============================================================\n",
            "Processing language 4/6: spanish\n",
            "============================================================\n",
            "  spanish: 30/101 files\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
            "Note: Illegal Audio-MPEG-Header 0x77bd7d56 at offset 945978.\n",
            "Note: Trying to resync...\n",
            "Note: Skipped 134 bytes in input.\n",
            "Note: Illegal Audio-MPEG-Header 0x0e9b81e3 at offset 1082816.\n",
            "Note: Trying to resync...\n",
            "Note: Skipped 98 bytes in input.\n",
            "Note: Illegal Audio-MPEG-Header 0xd30bc33e at offset 3202978.\n",
            "Note: Trying to resync...\n",
            "Note: Skipped 152 bytes in input.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  spanish: 100/101 files\n",
            "✓ Completed spanish\n",
            "\n",
            "============================================================\n",
            "Processing language 5/6: hindi\n",
            "============================================================\n",
            "  hindi: 80/81 files\n",
            "✓ Completed hindi\n",
            "\n",
            "============================================================\n",
            "Processing language 6/6: pidgin\n",
            "============================================================\n",
            "  pidgin: 60/100 files\r"
          ]
        }
      ],
      "source": [
        "print(\"Starting optimized preprocessing...\")\n",
        "metadata = build_dataset_by_language_batch()\n",
        "\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(f\"Metadata: {metadata}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-arm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
